{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpjv1XrCcn6RlgG79sHlDy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedBaari/Deep-Learning-Essentials/blob/main/8%20-%20Image%20Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5B5n9egYjkC",
        "outputId": "7eea0374-03ee-4d13-9aec-b48050864dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading VGG16...\n",
            "✓ VGG16 loaded\n",
            "\n",
            "Extracting features...\n",
            "Found 8091 images\n",
            "  50/500...\n",
            "  100/500...\n",
            "  150/500...\n",
            "  200/500...\n",
            "  250/500...\n",
            "  300/500...\n",
            "  350/500...\n",
            "  400/500...\n",
            "  450/500...\n",
            "  500/500...\n",
            "✓ Extracted features for 500 images\n",
            "\n",
            "Loading captions...\n",
            "✓ Loaded captions for 500 images\n",
            "✓ Vocab: 2202, Max: 36\n",
            "\n",
            "Creating sequences...\n",
            "✓ Created 29623 sequences\n",
            "\n",
            "Building model...\n",
            "✓ Model built\n",
            "\n",
            "Training...\n",
            "Epoch 1/5\n",
            "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 191ms/step - loss: 5.2475\n",
            "Epoch 2/5\n",
            "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 197ms/step - loss: 3.7976\n",
            "Epoch 3/5\n",
            "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 196ms/step - loss: 3.3128\n",
            "Epoch 4/5\n",
            "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 184ms/step - loss: 2.9149\n",
            "Epoch 5/5\n",
            "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 190ms/step - loss: 2.5919\n",
            "\n",
            "✓ Generated: a white dog is jumping after a ball in a ball\n",
            "✓ Actual: startseq a dog prepares to catch a thrown object in a field with nearby cars . endseq\n"
          ]
        }
      ],
      "source": [
        "# FIXED: Proper VGG16 initialization and error handling\n",
        "import os, numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, add\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Paths\n",
        "img_dir = '/content/Images/Images/'\n",
        "caption_file = '/content/Images/captions.txt'\n",
        "\n",
        "# 1. BUILD VGG16 PROPERLY (ONCE!)\n",
        "print(\"Loading VGG16...\")\n",
        "base_model = VGG16(weights='imagenet')\n",
        "vgg = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
        "print(\"✓ VGG16 loaded\")\n",
        "\n",
        "# 2. EXTRACT FEATURES\n",
        "print(\"\\nExtracting features...\")\n",
        "features = {}\n",
        "img_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]\n",
        "print(f\"Found {len(img_files)} images\")\n",
        "\n",
        "for i, f in enumerate(img_files[:500]):  # Limit to 500 for speed\n",
        "    if (i+1) % 50 == 0:\n",
        "        print(f\"  {i+1}/500...\")\n",
        "\n",
        "    try:\n",
        "        img_path = os.path.join(img_dir, f)\n",
        "        img = image.load_img(img_path, target_size=(224, 224))\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = preprocess_input(img_array)\n",
        "\n",
        "        feature = vgg.predict(img_array, verbose=0)\n",
        "        img_id = f.split('.')[0]\n",
        "        features[img_id] = feature[0]\n",
        "    except Exception as e:\n",
        "        # Skip corrupted images silently\n",
        "        continue\n",
        "\n",
        "print(f\"✓ Extracted features for {len(features)} images\")\n",
        "\n",
        "if len(features) == 0:\n",
        "    raise Exception(\"No features extracted! Check images.\")\n",
        "\n",
        "# 3. LOAD CAPTIONS\n",
        "print(\"\\nLoading captions...\")\n",
        "captions = {}\n",
        "\n",
        "with open(caption_file, 'r') as f:\n",
        "    lines = f.read().split('\\n')[1:]\n",
        "\n",
        "for line in lines:\n",
        "    if len(line) < 2: continue\n",
        "    parts = line.split(',', 1)\n",
        "    if len(parts) < 2: continue\n",
        "\n",
        "    img_id = parts[0].split('.')[0]\n",
        "\n",
        "    if img_id in features:\n",
        "        cap = 'startseq ' + parts[1].lower().strip() + ' endseq'\n",
        "        captions.setdefault(img_id, []).append(cap)\n",
        "\n",
        "print(f\"✓ Loaded captions for {len(captions)} images\")\n",
        "\n",
        "# 4. TOKENIZE\n",
        "all_caps = [c for caps in captions.values() for c in caps]\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(all_caps)\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "max_len = max(len(c.split()) for c in all_caps)\n",
        "\n",
        "print(f\"✓ Vocab: {vocab_size}, Max: {max_len}\")\n",
        "\n",
        "# 5. CREATE SEQUENCES\n",
        "print(\"\\nCreating sequences...\")\n",
        "X1, X2, y = [], [], []\n",
        "for img_id, caps in captions.items():\n",
        "    for cap in caps:\n",
        "        seq = tok.texts_to_sequences([cap])[0]\n",
        "        for i in range(1, len(seq)):\n",
        "            X1.append(features[img_id])\n",
        "            X2.append(pad_sequences([seq[:i]], maxlen=max_len)[0])\n",
        "            y.append(to_categorical([seq[i]], num_classes=vocab_size)[0])\n",
        "\n",
        "X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
        "print(f\"✓ Created {len(X1)} sequences\")\n",
        "\n",
        "# 6. BUILD MODEL\n",
        "print(\"\\nBuilding model...\")\n",
        "inp1 = Input(shape=(4096,))\n",
        "inp2 = Input(shape=(max_len,))\n",
        "\n",
        "fe = Dense(256, activation='relu')(inp1)\n",
        "se = Embedding(vocab_size, 256, mask_zero=True)(inp2)\n",
        "se = LSTM(256)(se)\n",
        "\n",
        "decoder = add([fe, se])\n",
        "decoder = Dense(256, activation='relu')(decoder)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "model = Model(inputs=[inp1, inp2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "print(\"✓ Model built\")\n",
        "\n",
        "# 7. TRAIN\n",
        "print(\"\\nTraining...\")\n",
        "model.fit([X1, X2], y, epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "# 8. GENERATE CAPTION\n",
        "def gen_cap(img_id):\n",
        "    text = 'startseq'\n",
        "    for _ in range(max_len):\n",
        "        seq = pad_sequences([tok.texts_to_sequences([text])[0]], maxlen=max_len)\n",
        "        pred = np.argmax(model.predict([features[img_id].reshape(1,-1), seq], verbose=0))\n",
        "        word = [k for k,v in tok.word_index.items() if v==pred]\n",
        "        if not word or word[0]=='endseq': break\n",
        "        text += ' ' + word[0]\n",
        "    return text.replace('startseq','').strip()\n",
        "\n",
        "# 9. TEST\n",
        "test_id = list(captions.keys())[0]\n",
        "print(f\"\\n✓ Generated: {gen_cap(test_id)}\")\n",
        "print(f\"✓ Actual: {captions[test_id][0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "439257aa",
        "outputId": "c7794b29-0c2d-4ac6-8d9c-b010223bf782"
      },
      "source": [
        "# Download from awsaf49's repository\n",
        "!wget \"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr8k.zip\"\n",
        "!unzip -q flickr8k.zip -d Images/\n",
        "!rm flickr8k.zip\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-29 03:34:32--  https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr8k.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/753516996/d7c62b13-1e50-40ea-8fae-f34a44b1695f?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-29T04%3A18%3A38Z&rscd=attachment%3B+filename%3Dflickr8k.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-29T03%3A17%3A43Z&ske=2025-10-29T04%3A18%3A38Z&sks=b&skv=2018-11-09&sig=RJeB5MnUDNYNWBhFaS3ICJz3o2GNmNNZVlbFNIgo8kE%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MTcxMjQ3MiwibmJmIjoxNzYxNzA4ODcyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.KNQqVKfmliRsG6HV0D1_vbYDyY8mAVr_NT1JpxypiPs&response-content-disposition=attachment%3B%20filename%3Dflickr8k.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-10-29 03:34:32--  https://release-assets.githubusercontent.com/github-production-release-asset/753516996/d7c62b13-1e50-40ea-8fae-f34a44b1695f?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-29T04%3A18%3A38Z&rscd=attachment%3B+filename%3Dflickr8k.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-29T03%3A17%3A43Z&ske=2025-10-29T04%3A18%3A38Z&sks=b&skv=2018-11-09&sig=RJeB5MnUDNYNWBhFaS3ICJz3o2GNmNNZVlbFNIgo8kE%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MTcxMjQ3MiwibmJmIjoxNzYxNzA4ODcyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.KNQqVKfmliRsG6HV0D1_vbYDyY8mAVr_NT1JpxypiPs&response-content-disposition=attachment%3B%20filename%3Dflickr8k.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1112971163 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘flickr8k.zip’\n",
            "\n",
            "flickr8k.zip        100%[===================>]   1.04G  97.6MB/s    in 9.3s    \n",
            "\n",
            "2025-10-29 03:34:41 (114 MB/s) - ‘flickr8k.zip’ saved [1112971163/1112971163]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2d3b66c",
        "outputId": "3e6694e2-43b2-4c62-d256-828083f277af"
      },
      "source": [
        "import os\n",
        "print(os.listdir('/content'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'Images', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS FIRST TO DEBUG\n",
        "import os\n",
        "\n",
        "# Check what's in your content folder\n",
        "print(\"Contents of /content/:\")\n",
        "print(os.listdir('/content/'))\n",
        "\n",
        "# Check if Images folder exists\n",
        "if os.path.exists('/content/Images'):\n",
        "    print(\"\\n✓ Images folder exists\")\n",
        "    files = os.listdir('/content/Images/')\n",
        "    print(f\"Files in Images folder: {len(files)}\")\n",
        "    print(f\"First 10 files: {files[:10]}\")\n",
        "else:\n",
        "    print(\"\\n✗ Images folder NOT found!\")\n",
        "    print(\"Available folders:\", [d for d in os.listdir('/content/') if os.path.isdir(os.path.join('/content/', d))])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KryrUnudFw99",
        "outputId": "885f7932-3a4b-4a67-f2c0-d994f4eb094d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of /content/:\n",
            "['.config', 'Images', 'sample_data']\n",
            "\n",
            "✓ Images folder exists\n",
            "Files in Images folder: 2\n",
            "First 10 files: ['captions.txt', 'Images']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0RBzMWLlZanT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}